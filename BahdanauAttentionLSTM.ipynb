{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BahdanauAttentionLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/twrZJZIsl//+2cPFGpW7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xessen/LSTM-Variants/blob/main/BahdanauAttentionLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A_HvY1_qUWs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,Input,TextVectorization\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "Eap8RB26qaUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "drive.mount('/content/gdrive')\n",
        "!cp '/content/gdrive/MyDrive/biletler/comments_data (5).db' '/content/'\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "Bw3q7F-eqewm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn=sqlite3.connect('/content/comments_data (5).db')\n",
        "sql_q=pd.read_sql_query(\"SELECT parent_data,data FROM comments\",conn)\n",
        "df=pd.DataFrame(sql_q,columns=['parent_data','data'])\n",
        "df['parent_data']=df['parent_data'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "df['data']=df['data'].str.replace('[{}]'.format(string.punctuation), '')"
      ],
      "metadata": {
        "id": "lEwlmqxKqio-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_=TextVectorization(max_tokens=8000,output_sequence_length=550)\n",
        "\n",
        "\n",
        "\n",
        "vectorizer_.adapt(df['parent_data'])\n",
        "\n",
        "voc_ = vectorizer_.get_vocabulary()\n",
        "voc_.append('specialstartkey')\n",
        "voc_.append('specialstopkey')\n",
        "\n",
        "vectorizer=TextVectorization(max_tokens=8002,output_sequence_length=550,vocabulary=voc_)\n",
        "voc=vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n"
      ],
      "metadata": {
        "id": "_2niHUouqjxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_index={}\n",
        "f=open(\"/content/glove.6B.50d.txt\",\"r\")\n",
        "for line in f:\n",
        "  word,coefs=line.split(maxsplit=1)\n",
        "  coefs=np.fromstring(coefs,\"f\",sep=\" \")\n",
        "  embedding_index[word]=coefs\n"
      ],
      "metadata": {
        "id": "IoB86c-TqlIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc) \n",
        "embedding_dim = 50\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "y0gBV1TcqmNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,units):\n",
        "    super(AttentionLayer,self).__init__()\n",
        "    self.W1=Dense(units)\n",
        "    self.W2=Dense(units)\n",
        "    self.V=Dense(1)\n",
        "    \n",
        "\n",
        "\n",
        "    def call(self,query,values):\n",
        "\n",
        "      #query=tf.expand_dims(query,1)\n",
        "\n",
        "      score=self.V(tf.nn.tanh(self.W1(query)+self.W2(values)))\n",
        "\n",
        "      attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "      context_vector = attention_weights * values\n",
        "      \n",
        "      context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "      #(1,256)\n",
        "      return context_vector\n",
        "      "
      ],
      "metadata": {
        "id": "JKjxsF6Gr-QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionNLP(Model):\n",
        "  def __init__(self,LSTMshape,num_tokens,embedding_dim,embedding_matrix,max_len):\n",
        "    super(AttentionNLP,self).__init__()\n",
        "    np_config.enable_numpy_behavior()\n",
        "\n",
        "    self.max_len=max_len\n",
        "\n",
        "    self.encoder_input=Input(shape=(max_len,))\n",
        "    self.e_embed_layer=Embedding(num_tokens,embedding_dim,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False,mask_zero=True)\n",
        "    self.encoder_lstm=LSTM(LSTMshape,return_state=True,return_sequences=True)\n",
        "\n",
        "\n",
        "    self.encoder_lstm1=LSTM(LSTMshape,return_state=True)\n",
        "\n",
        "    self.attention_layer=AttentionLayer(LSTMshape)\n",
        "\n",
        "    self.decoder_input=Input(shape=(1,))\n",
        "    self.d_embed_layer=Embedding(num_tokens,embedding_dim,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False,mask_zero=True)\n",
        "\n",
        "    self.decoder_lstm=LSTM(LSTMshape,return_state=True,return_sequences=True)\n",
        "\n",
        "    self.decoder_lstm1=LSTM(LSTMshape,return_state=True)\n",
        "\n",
        "    self.decoder_dense=Dense(num_tokens)\n",
        "  \n",
        "  def call(self,inputs):\n",
        "    all_outputs=[]\n",
        "    encoder_embed_output=self.e_embed_layer(inputs)\n",
        "\n",
        "    LSTM_o,state_h,state_c=self.encoder_lstm(encoder_embed_output)\n",
        "    encoder_states=[state_h,state_c]\n",
        "    LSTM_o1,state_h1,state_c1=self.encoder_lstm1(LSTM_o,initial_state=encoder_states)\n",
        "    encoder_states_1=[state_h1,state_c1]\n",
        "\n",
        "    attention_in=state_h1\n",
        "    decoder_in=tf.cast(np.zeros((1,256)),dtype=tf.float32)\n",
        "    decoder_initial_states=encoder_states_1\n",
        "    for _ in range(self.max_len):\n",
        "      context_vector=self.attention_layer(attention_in,LSTM_o1)\n",
        "      #context_vector = tf.expand_dims(context_vector, 1)\n",
        "\n",
        "\n",
        "      l_inp=tf.expand_dims(tf.concat([decoder_in,context_vector],-1),1)\n",
        "\n",
        "      decoder_o,dstate_h,dstate_c=self.decoder_lstm(l_inp,initial_state=decoder_initial_states)\n",
        "      decoder_states=[dstate_h,dstate_c]\n",
        "\n",
        "\n",
        "\n",
        "      decoder_o1,dstate_h1,dstate_c1=self.decoder_lstm1(decoder_o,initial_state=decoder_states)\n",
        "      decoder_initial_states=[dstate_h1,dstate_c1]\n",
        "      decoder_in=decoder_initial_states[0]\n",
        "      attention_in=decoder_initial_states[0]\n",
        "\n",
        "      dec_out=self.decoder_dense(decoder_o1)\n",
        "      d_out=tf.math.argmax(dec_out)\n",
        "      all_outputs.append(d_out)\n",
        "\n",
        "\n",
        "    return all_outputs\n",
        "\n",
        "  def build_graph(self):\n",
        "    x=Input(shape=(550,))\n",
        "\n",
        "    return AttentionNLP(inputs=x,outputs=self.call(x))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eYl3RqYmuvqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model=AttentionNLP(256,8002,50,embedding_matrix,550)\n",
        "my_model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ju6ZFEK_udbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with strategy.scope():\n",
        "X_train, X_test, y_train, y_test=train_test_split(df['parent_data'],df['data'],test_size=0.2)\n",
        "x_encoder_in=vectorizer(X_train)\n",
        "y_train='specialstartkey '+y_train+' specialstopkey'\n",
        "x_decoder_in=vectorizer(y_train)\n",
        "y_decoder_out=vectorizer(y_train)\n",
        "  "
      ],
      "metadata": {
        "id": "12v9s-iawvCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.fit(x_encoder_in,y_decoder_out,batch_size=64,epochs=1)\n"
      ],
      "metadata": {
        "id": "Je-q713vu7bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JcJx4VJlwqIG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
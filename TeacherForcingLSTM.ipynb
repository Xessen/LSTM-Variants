{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xessen/TeacherForcingLSTM/blob/main/TeacherForcingLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htblXraCDjJb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,Input,TextVectorization\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "import nltk\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "ucS7PwXnKhy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdEWykr0NwFR"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "drive.mount('/content/gdrive')\n",
        "!cp '/content/gdrive/MyDrive/biletler/comments_data (5).db' '/content/'\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjHD7rHaGke0"
      },
      "outputs": [],
      "source": [
        "conn=sqlite3.connect('/content/comments_data (5).db')\n",
        "sql_q=pd.read_sql_query(\"SELECT parent_data,data FROM comments\",conn)\n",
        "df=pd.DataFrame(sql_q,columns=['parent_data','data'])\n",
        "df['parent_data']=df['parent_data'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "df['data']=df['data'].str.replace('[{}]'.format(string.punctuation), '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xatPtOpSM4ns"
      },
      "outputs": [],
      "source": [
        "vectorizer_=TextVectorization(max_tokens=8000,output_sequence_length=550)\n",
        "\n",
        "\n",
        "\n",
        "vectorizer_.adapt(df['parent_data'])\n",
        "\n",
        "voc_ = vectorizer_.get_vocabulary()\n",
        "voc_.append('specialstartkey')\n",
        "voc_.append('specialstopkey')\n",
        "\n",
        "vectorizer=TextVectorization(max_tokens=8002,output_sequence_length=550,vocabulary=voc_)\n",
        "voc=vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8FlkJLlQVYt"
      },
      "outputs": [],
      "source": [
        "\n",
        "embedding_index={}\n",
        "f=open(\"/content/glove.6B.50d.txt\",\"r\")\n",
        "for line in f:\n",
        "  word,coefs=line.split(maxsplit=1)\n",
        "  coefs=np.fromstring(coefs,\"f\",sep=\" \")\n",
        "  embedding_index[word]=coefs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-l8znthRmib"
      },
      "outputs": [],
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 50\n",
        "\n",
        "\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "#with strategy.scope():\n",
        "class TeacherForcingNLP(Model):\n",
        "  def __init__(self,num_tokens,embedding_dim,embedding_matrix,max_len):\n",
        "    super(TeacherForcingNLP,self).__init__()\n",
        "    np_config.enable_numpy_behavior()\n",
        "\n",
        "    self.encoder_input=Input(shape=(max_len,))\n",
        "    self.e_embed_layer=Embedding(num_tokens,embedding_dim,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False,mask_zero=True)\n",
        "    self.encoder_lstm=LSTM(256,return_state=True,return_sequences=True)\n",
        "\n",
        "\n",
        "    self.encoder_lstm1=LSTM(256,return_state=True)\n",
        "\n",
        "\n",
        "    self.decoder_input=Input(shape=(None,))\n",
        "    self.d_embed_layer=Embedding(num_tokens,embedding_dim,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),name=\"decoder_embed\",trainable=False,mask_zero=True)\n",
        "\n",
        "    self.decoder_lstm=LSTM(256,return_state=True,return_sequences=True,name=\"decoder_lstm\")\n",
        "\n",
        "\n",
        "    self.decoder_lstm1=LSTM(256,return_state=True,return_sequences=True,name=\"decoder_lstm1\")\n",
        "\n",
        "    self.decoder_dense=Dense(num_tokens,activation=\"softmax\",name='decoder_dense')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def call(self,inputs):\n",
        "    encoder_embed_output=self.e_embed_layer(inputs[0])\n",
        "\n",
        "    LSTM_o,state_h,state_c=self.encoder_lstm(encoder_embed_output)\n",
        "    encoder_states=[state_h,state_c]\n",
        "    LSTM_o1,state_h1,state_c1=self.encoder_lstm1(LSTM_o,initial_state=encoder_states)\n",
        "    encoder_states_1=[state_h1,state_c1]\n",
        "    decoder_embed_output=self.d_embed_layer(inputs[1])\n",
        "\n",
        "    decoder_outputs,d_state_h,d_state_c=self.decoder_lstm(decoder_embed_output,initial_state=encoder_states_1)\n",
        "    decoder_states=[d_state_h,d_state_c]\n",
        "\n",
        "    decoder_outputs1,_,_=self.decoder_lstm1(decoder_outputs,initial_state=decoder_states)\n",
        "\n",
        "    decoder_output=self.decoder_dense(decoder_outputs1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return decoder_output\n",
        "  def build_graph(self):\n",
        "    x = Input(shape=(550,))\n",
        "    y=Input(shape=(550,))\n",
        "    return Model(inputs=[x,y], outputs=self.call([x,y]))\n",
        "  def inference_encoder(self):\n",
        "    #einp=Input(shape=(550,))\n",
        "\n",
        "    encoder_embed_output=self.e_embed_layer(self.encoder_input)\n",
        "\n",
        "    LSTM_o,state_h,state_c=self.encoder_lstm(encoder_embed_output)\n",
        "    encoder_states=[state_h,state_c]\n",
        "\n",
        "    LSTM_o1,state_h1,state_c1=self.encoder_lstm1(LSTM_o,initial_state=[encoder_states[0],encoder_states[1]])\n",
        "    encoder_states_1=[state_h1,state_c1]\n",
        "    encoder_model=Model(self.encoder_input,encoder_states_1)\n",
        "\n",
        "\n",
        "\n",
        "    return encoder_model\n",
        "\n",
        "  def inference_decoder(self):\n",
        "\n",
        "    dinp=Input(shape=(1,))\n",
        "\n",
        "    decoder_state_input_h = Input(shape=(256,))\n",
        "    decoder_state_input_c = Input(shape=(256,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    i_decoder_embed_output=self.d_embed_layer(dinp)\n",
        "\n",
        "    i_decoder_outputs, i_state_h, i_state_c = self.decoder_lstm(\n",
        "    i_decoder_embed_output, initial_state=decoder_states_inputs)\n",
        "    i_decoder_states = [i_state_h, i_state_c]\n",
        "\n",
        "    i_decoder_outputs1,i_state_h1,i_state_c1=self.decoder_lstm1(i_decoder_outputs,initial_state=i_decoder_states)\n",
        "    i_decoder_states1=[i_state_h1,i_state_c1]\n",
        "    \n",
        "    i_dec_out=self.decoder_dense(i_decoder_outputs1)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    decoder_model=Model([dinp,decoder_states_inputs],[i_dec_out,i_decoder_states1])\n",
        "    return decoder_model\n",
        "  def inference(self,input_seq):\n",
        "\n",
        "    encoder_model=self.inference_encoder()\n",
        "    initial_states=encoder_model.predict(input_seq.reshape(1,550))\n",
        "    initial_states=tf.convert_to_tensor(initial_states[0]),tf.convert_to_tensor(initial_states[1])\n",
        "    \n",
        "\n",
        "    stop_condition=False\n",
        "    dec_out=8000\n",
        "    decoded_seq=[]\n",
        "    decoder_model=self.inference_decoder()\n",
        "\n",
        "    while not stop_condition:\n",
        "      dec_out,initial_states=decoder_model.predict([np.array(dec_out).reshape(1,1),initial_states])\n",
        "      dec_out=np.argmax(dec_out[0,0,:])\n",
        "      decoded_seq.append(dec_out)\n",
        "      if len(decoded_seq)>25 or (dec_out==8001 and len(decoded_seq)>5):\n",
        "        stop_condition=True\n",
        "    return decoded_seq\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CgI1NOcMDtBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with strategy.scope():\n",
        "max_len=550\n",
        "a_model=TeacherForcingNLP(8002,50,[],max_len)\n",
        "a_model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=1, patience=2)"
      ],
      "metadata": {
        "id": "UYv1Qn2qNeuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with strategy.scope():\n",
        "X_train, X_test, y_train, y_test=train_test_split(df['parent_data'],df['data'],test_size=0.2)\n",
        "x_encoder_in=vectorizer(X_train)\n",
        "y_train='specialstartkey '+y_train+' specialstopkey'\n",
        "x_decoder_in=vectorizer(y_train)\n",
        "y_decoder_out=vectorizer(y_train)\n",
        "  "
      ],
      "metadata": {
        "id": "0gz2CWMfickH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with strategy.scope():\n",
        "a_model.fit([x_encoder_in[200:300],x_decoder_in[200:300]],y_decoder_out[200:300],batch_size=64,epochs=1,callbacks=[es])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDYcS0CuKu5E",
        "outputId": "50886854-1d16-446b-bcc5-a5a425856b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 20s 425ms/step - loss: 0.7069 - accuracy: 0.0437\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efecd45ae90>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TeacherForcingLSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKQ9B6yredxqgwyC/fjuXo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}